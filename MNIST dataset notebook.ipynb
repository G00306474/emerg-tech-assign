{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading bytes from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KMora\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras as kr\n",
    "import sklearn.preprocessing as pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://docs.python.org/3/library/gzip.html\n",
    "\n",
    "\n",
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    file_content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x00\\x00\\x08\\x03'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_content[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Little and big endian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2051"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adapted from: https://stackoverflow.com/questions/51220161/how-to-convert-from-bytes-to-int\n",
    "\n",
    "int.from_bytes(file_content[0:4], byteorder='big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(file_content[4:8], byteorder='big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(file_content[8:12], byteorder='big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(file_content[12:16], byteorder='big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(file_content[278:279], byteorder='big')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = file_content[16:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "image = ~np.array(list(file_content[16:800])).reshape(28,28).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x234d30f42e8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://docs.python.org/3/library/gzip.html\n",
    "\n",
    "\n",
    "\n",
    "with gzip.open('data/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(labels[8:9], byteorder=\"big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "\n",
    "# Start a neural network, building it by layers.\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# Add a hidden layer with 1000 neurons and an input layer with 784.\n",
    "model.add(kr.layers.Dense(units=1000, activation='relu', input_dim=784))\n",
    "model.add(Dropout(0.2)) #Dropout is a technique where randomly selected neurons are ignored during training. \n",
    "# They are “dropped-out” randomly. \n",
    "# This means that their contribution to the activation of downstream neurons is temporally removed on the \n",
    "# forward pass and any weight updates are not applied to the neuron on the backward pass.\n",
    "model.add(kr.layers.Dense(units=1000, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(kr.layers.Dense(units=1000, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(kr.layers.Dense(units=1000, activation='relu'))\n",
    "\n",
    "\n",
    "# Add a 10 neuron output layer.\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "\n",
    "# Build the graph.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('data/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    train_img = f.read()\n",
    "\n",
    "with gzip.open('data/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    train_lbl = f.read()\n",
    "    \n",
    "train_img = ~np.array(list(train_img[16:])).reshape(60000, 28, 28).astype(np.uint8)\n",
    "train_lbl =  np.array(list(train_lbl[ 8:])).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = train_img.reshape(60000, 784)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 [0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# For encoding categorical variables.\n",
    "\n",
    "\n",
    "encoder = pre.LabelBinarizer()\n",
    "encoder.fit(train_lbl)\n",
    "outputs = encoder.transform(train_lbl)\n",
    "\n",
    "print(train_lbl[0], outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 1.1246 - acc: 0.6486\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 0.4704 - acc: 0.8571\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 90s 2ms/step - loss: 0.3797 - acc: 0.8841\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.3328 - acc: 0.8994\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.2971 - acc: 0.9088\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 92s 2ms/step - loss: 0.2686 - acc: 0.9187\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.2468 - acc: 0.9254\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.2269 - acc: 0.9315\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.2096 - acc: 0.9368\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.1966 - acc: 0.9409\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.1840 - acc: 0.9439\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 0.1755 - acc: 0.9470\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.1639 - acc: 0.9493\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.1551 - acc: 0.9529\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.1507 - acc: 0.9545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x234d285cb00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(inputs, outputs, epochs=15, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the current model\n",
    "kr.models.save_model(\n",
    "    model,\n",
    "    \"savedModel.h5py\",\n",
    "    overwrite=True,\n",
    "    include_optimizer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the model file exists load it\n",
    "if os.path.isfile('savedModel.h5py'): \n",
    "    model = kr.models.load_model('savedModel.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    test_img = f.read()\n",
    "\n",
    "with gzip.open('data/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    test_lbl = f.read()\n",
    "    \n",
    "test_img = ~np.array(list(test_img[16:])).reshape(10000, 784).astype(np.uint8)\n",
    "test_lbl =  np.array(list(test_lbl[ 8:])).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9637"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(encoder.inverse_transform(model.predict(test_img)) == test_lbl).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_5 to have shape (10,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-d22cd76f5f56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#model.predict(test_img)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_lbl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[0;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1102\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_5 to have shape (10,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "#model.evaluate(test_img, test_lbl, verbose=0 )\n",
    "#model.predict(test_img)\n",
    "\n",
    "score = model.evaluate(test_img, test_lbl, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
